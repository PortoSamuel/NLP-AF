{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "plt.style.use('default')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O database escolhido é está disponível no kaggle como: Coronavirus tweets NLP - Text Classification (https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification).\n",
    "\n",
    "Esse database contém tweets postados na pandemia, junto de data, localização e classificação do post. Sendo a classificação uma variável categórica ordinal (Extremely Negative, Negative, Neutral, Positive e Extremely Positive) onde os tweets negativos passam informações de insegurança e medo causado por conta do vírus e os positivos são o oposto.\n",
    "\n",
    "Por motivos de agilidade de implementação, testes e análises neste documentos vamos considerar apenas duas categorias Negativo que encapsula as originais categorias Extremely Negative e Negative; e Positivo correspondendo as categorias Positive, Extremely Positive e Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Corona_NLP_train.csv', encoding = 'latin1')\n",
    "test = pd.read_csv('Corona_NLP_test.csv', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorias(x):\n",
    "    if x ==  \"Extremely Positive\":\n",
    "        return \"1\"\n",
    "    elif x == \"Extremely Negative\":\n",
    "        return \"0\"\n",
    "    elif x == \"Negative\":\n",
    "        return \"0\"\n",
    "    elif x ==  \"Positive\":\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "train['label']=train['Sentiment'].apply(lambda x:categorias(x))\n",
    "test['label']=test['Sentiment'].apply(lambda x:categorias(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Vamos limpar a fontes de dados para podermos ter um melhor desempenho na modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o filtro de stop words, pontuações, POS e padroniza o texto\n",
    "def filter_serie(serie):\n",
    "\n",
    "    # Padrões regex que serão filtrados nos comentarios\n",
    "    hyperlink = '(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+'\n",
    "    date = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n",
    "    dinheiro = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
    "    negacao = '([nN][ãÃaA][oO]|[ñÑ]| [nN] )'\n",
    "    mencao = '@\\w+'\n",
    "    hash = '#\\w+'\n",
    "    emoji = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    sw = stopwords.words('portuguese')\n",
    "    sw.append('\\n')\n",
    "    sw.append('\\r')\n",
    "\n",
    "    # Aplica o regex para remover links\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(hyperlink, ' link ', word) for word in x.split()]))\n",
    "\n",
    "    # Deixa todos os reviews em lower case\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [word for word in x.lower().split() if word not in (sw)]))\n",
    "\n",
    "    # Aplica o regex para remover mencoes\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(mencao, '', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica o regex para remover hashtags\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(hash, '', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica o regex para remover emojis\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(emoji, '', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica o regex para remover datas\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(date, ' date ', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica o regex para remover valores monetários\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(dinheiro, ' money ', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica o regex para padronizar negações\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub(negacao, ' not ', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica regex para remover caracteres especiais\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub('\\W', ' ', word) for word in x.split()]))\n",
    "\n",
    "    # Remove as pontuações\n",
    "    serie = serie.apply(lambda x: ''.join(\n",
    "        [word for word in x if word not in (string.punctuation)]))\n",
    "\n",
    "    # Aplica regex para remover espaçamento a mais no início de cada review\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub('\\s+', ' ', word) for word in x.split()]))\n",
    "\n",
    "    # Aplica regex para remover espaçamento a mais no final de cada review\n",
    "    serie = serie.apply(lambda x: ' '.join(\n",
    "        [re.sub('[ \\t]+$', '', word) for word in x.split()]))\n",
    "\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = filter_serie(train['OriginalTweet'])\n",
    "test['text'] = filter_serie(test['OriginalTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['OriginalTweet','text', 'label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_test = vectorizer.transform(test['text'])\n",
    "\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aulas:\n",
    "11\n",
    "12\n",
    "17\n",
    "18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c00290ae0f647b852925298c0fa9f685b24793fde77f3b1ecdb20a685ed4d9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
